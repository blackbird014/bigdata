# Import and create a new SQLContext 
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)

# Read the country CSV file into an RDD.
country_lines = sc.textFile('file:///home/cloudera/Downloads/big-data-3/final-project/country-list.csv')
country_lines.count()

# Convert each line into a pair of words
#pair= country_lines.flatMap(lambda line: line.split(','))

# Convert each pair of words into a tuple
#from IPython.core.debugger import Tracer
#Tracer()()
#country_tuples = pair.map(lambda word : (word,1))
#pair.count()
#country_tuples.count()

# Convert each line into a pair of wordsthe 2 steps above in one shot
#pair= country_lines.flatMap(lambda line: line.split(','))
country_tuples= country_lines.map(lambda line:tuple(line.split(","))).collect()

# Create the DataFrame, look at schema and contents
countryDF = sqlContext.createDataFrame(country_tuples, ["country", "code"])
countryDF.printSchema()
countryDF.take(3)

# Read tweets CSV file into RDD of lines
tweet_lines = sc.textFile('file:///home/cloudera/Downloads/big-data-3/final-project/users2.csv')
tweet_lines.count()

# Just a scrachpad test for string handling in python
test="test,vero"
test.split(",")[0] =="test"

# Clean the data: some tweets are empty. Remove the empty tweets using filter() 
tweet_lines=tweet_lines.filter(lambda line: (line.split(",")[0]!=""))
tweet_lines.count()
#tweet_lines_splitted= tweet_lines.flatMap(lambda line: line.split(','))
#tweet_lines_splitted.count()

# Perform WordCount on the cleaned tweet texts. (note: this is several lines.)
words = tweet_lines.flatMap(lambda line: line.split(' '))
words_tuples = words.map(lambda word: (word,1))
#pair.count()
words_tuples.count()

